{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95468c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "from tsfresh import select_features, extract_features\n",
    "from tsfresh.feature_extraction import EfficientFCParameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import multiprocessing\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ac590",
   "metadata": {},
   "source": [
    "#### Step 1: Parsear times series de train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3326c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_variable_from_file('train', 'x')\n",
    "x_train_df = pd.DataFrame(x_train[:, 0, :])\n",
    "x_train_parsed = parsear_time_series(x_train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0911df1",
   "metadata": {},
   "source": [
    "#### Step 2: Parsear labels de train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80a309ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = get_variable_from_file('train', 'y')\n",
    "y_train_parsed = parsear_y(x=x_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b266e1e",
   "metadata": {},
   "source": [
    "#### Step 3: Parsear times series de test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74c31313",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = get_variable_from_file('test', 'x')\n",
    "x_test_df = pd.DataFrame(x_test[:, 0, :])\n",
    "x_test_parsed = parsear_time_series(x_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8562c",
   "metadata": {},
   "source": [
    "#### Step 4: Extraer features de train parseado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17c8ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = extract_all_features(x_train_parsed) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbeeaf2",
   "metadata": {},
   "source": [
    "#### Step 5: Extraer features de test parseado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "401253ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = extract_all_features(x_test_parsed) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae106127",
   "metadata": {},
   "source": [
    "#### Step 6: Filtrar features de los features de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bb0898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_filtered = select_features(features_train, y_train_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d402a2",
   "metadata": {},
   "source": [
    "#### Step 7: Filtrar features de los features de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d38a9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_filtered = features_test[features_train_filtered.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027b359",
   "metadata": {},
   "source": [
    "#### Step 8: Guardar todos los features y label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6683bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_train_x = \"./features_blocks/train_features_filtered.pkl\"\n",
    "output_path_train_y = \"./features_blocks/train_labels.pkl\"\n",
    "output_path_test_x = \"./features_blocks/test_features_filtered.pkl\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path_train_x), exist_ok=True)\n",
    "features_train_filtered.to_pickle(output_path_train_x, compression=\"xz\")\n",
    "os.makedirs(os.path.dirname(output_path_train_y), exist_ok=True)\n",
    "temporary_y_series =  pd.Series(y_train_parsed)\n",
    "temporary_y_series.to_pickle(output_path_train_y, compression=\"xz\")\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path_test_x), exist_ok=True)\n",
    "features_test_filtered.to_pickle(output_path_test_x, compression=\"xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcfa9b",
   "metadata": {},
   "source": [
    "#### Step 9: Entrenar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72d89db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Entrenando modelo: Tree\n",
      "✅ Mejor score: 0.717988163688921 con {'max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "features_train_filtered = pd.read_pickle(\"features_blocks/train_features_filtered.pkl\", compression=\"xz\")\n",
    "features_test_filtered = pd.read_pickle(\"features_blocks/test_features_filtered.pkl\", compression=\"xz\")\n",
    "y_train_parsed = pd.read_pickle(\"features_blocks/train_labels.pkl\", compression=\"xz\")\n",
    "modelsName = [\"tree\"]\n",
    "models = gridSearchCVOverModels(features_train_filtered.values, y_train_parsed.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b140aa5",
   "metadata": {},
   "source": [
    "#### Step 10: Predecir y guardar predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cf6c6347",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(modelsName)):\n",
    "    model = models[i]\n",
    "    y_pred = model.predict(features_test_filtered.values)\n",
    "    np.save(f\"predicciones_{modelsName[i]}_1.npy\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6538f",
   "metadata": {},
   "source": [
    "#### Step 11: Parsear predicciones a formato .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1d87723",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.load(\"predicciones_tree_1.npy\")\n",
    "df = pd.DataFrame({\n",
    "    \"ID\": np.arange(1, len(y_pred) + 1),\n",
    "    \"result\": y_pred\n",
    "})\n",
    "\n",
    "df.to_csv(\"predicciones_tree_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4b03c",
   "metadata": {},
   "source": [
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c12012e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_from_file(path, variable):\n",
    "    with h5py.File(f'{path}.h5', 'r') as f:\n",
    "        data = f[f'{variable}'][:]\n",
    "        return data\n",
    "modelos = {\n",
    "    \"Tree\": (\n",
    "        DecisionTree(),\n",
    "        {\"max_depth\": [6, 8, 10]}  # Puedes ajustar según el caso\n",
    "    ),\n",
    "}\n",
    "\n",
    "def gridSearchCVOverModels(features_train, y_blocks):\n",
    "    best_models = []\n",
    "    for nombre, (modelo, params) in modelos.items():\n",
    "        print(f\"\\n🔍 Entrenando modelo: {nombre}\")\n",
    "        grid = GridSearchCV(estimator=modelo, param_grid=params, cv=5, scoring='accuracy')\n",
    "        grid.fit(features_train, y_blocks)\n",
    "        print(f\"✅ Mejor score: {grid.best_score_} con {grid.best_params_}\")\n",
    "        best_models.append(grid.best_estimator_)\n",
    "    return best_models\n",
    "def contar_bloques_por_serie(longitud_serie, tam_bloque, solapamiento):\n",
    "    paso = tam_bloque - solapamiento\n",
    "    return (longitud_serie - tam_bloque) // paso + 1\n",
    "\n",
    "def parsear_y(x, y, tam_bloque = 500, solapamiento = 225):\n",
    "    longitud_serie = x.shape[2]\n",
    "    n_bloques_por_serie = contar_bloques_por_serie(longitud_serie, tam_bloque, solapamiento)\n",
    "    y_train_blocks = np.repeat(y, n_bloques_por_serie)\n",
    "    return y_train_blocks\n",
    "\n",
    "def parsear_time_series(dataset: pd.DataFrame, tam_bloque: int = 500, solapamiento: int = 225) -> pd.DataFrame:\n",
    "    paso = tam_bloque - solapamiento\n",
    "    parsed_frames = []\n",
    "    nuevo_id = 0\n",
    "    for _, serie in enumerate(dataset.values):\n",
    "        longitud = serie.size\n",
    "        if longitud < tam_bloque:\n",
    "            continue\n",
    "        for inicio in range(0, longitud - tam_bloque + 1, paso):\n",
    "            fin = inicio + tam_bloque\n",
    "            bloque = serie[inicio:fin]\n",
    "\n",
    "            parsed_frames.append(\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": nuevo_id,\n",
    "                        \"time\": np.arange(inicio, fin),  # índice absoluto\n",
    "                        \"valor\": bloque,\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            nuevo_id += 1\n",
    "    return pd.concat(parsed_frames, ignore_index=True)\n",
    "\n",
    "def parsear_time_series_without_blocks(dataset: pd.DataFrame, tam_bloque: int = 500, solapamiento: int = 225) -> pd.DataFrame:\n",
    "    paso = tam_bloque - solapamiento\n",
    "    parsed_frames = []\n",
    "    nuevo_id = 0\n",
    "    for _, serie in enumerate(dataset.values):\n",
    "        longitud = serie.size\n",
    "        if longitud < tam_bloque:\n",
    "            continue\n",
    "        for inicio in range(0, longitud - tam_bloque + 1, paso):\n",
    "            fin = inicio + tam_bloque\n",
    "            bloque = serie[inicio:fin]\n",
    "\n",
    "            parsed_frames.append(\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": nuevo_id,\n",
    "                        \"time\": np.arange(inicio, fin),  # índice absoluto\n",
    "                        \"valor\": bloque,\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            nuevo_id += 1\n",
    "    return pd.concat(parsed_frames, ignore_index=True)\n",
    "\n",
    "\n",
    "def extract_all_features(df: pd.DataFrame):\n",
    "    features_all = extract_features(df,\n",
    "                                    column_id=\"id\",\n",
    "                                    column_sort=\"time\",\n",
    "                                    default_fc_parameters=EfficientFCParameters(),\n",
    "                                    disable_progressbar=True,\n",
    "                                    n_jobs=max(\n",
    "                                        1, multiprocessing.cpu_count() - 1)\n",
    "                                    )\n",
    "    features_all = features_all.fillna(0)\n",
    "    return features_all\n",
    "\n",
    "def calculate_clusters_based_on_variance(column):\n",
    "    variance = np.var(column)\n",
    "    \n",
    "    if variance < 1:\n",
    "        return 2 \n",
    "    elif variance < 10:\n",
    "        return 3  \n",
    "    else:\n",
    "        return 4 \n",
    "    \n",
    "def discretize_with_kmeans(column):\n",
    "    n_clusters = calculate_clusters_based_on_variance(column)\n",
    "    \n",
    "    # Reshape para aplicar KMeans (n_samples, n_features)\n",
    "    column_reshaped = column.values.reshape(-1, 1)\n",
    "    \n",
    "    # Ajustamos KMeans a la columna con el número de clusters calculado\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(column_reshaped)\n",
    "    \n",
    "    # Asignar a cada valor el índice del cluster\n",
    "    return kmeans.labels_\n",
    "\n",
    "def discretize_all(dataset):\n",
    "    x_df_discretized = dataset\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        n_clusters = 3  \n",
    "        x_df_discretized[col] = discretize_with_kmeans(dataset[col])\n",
    "    return x_df_discretized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e8f2e",
   "metadata": {},
   "source": [
    "#### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ef56694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    \"\"\"Calcular la impureza Gini de un array de etiquetas\"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    # Proporción de cada clase\n",
    "    clases = np.unique(y)\n",
    "    probabilities = [np.mean(y == c) for c in clases]\n",
    "    gini = 1 - sum(p ** 2 for p in probabilities)\n",
    "    return gini\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"Calcular la entropía de un array de etiquetas\"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    # Proporción de cada clase\n",
    "    clases = np.unique(y)\n",
    "    probabilities = [np.mean(y == c) for c in clases]\n",
    "    # Usamos log2 para calcular la entropía\n",
    "    entropy_value = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "    return entropy_value\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, impurity_function=gini_impurity,\n",
    "                 n_bins=20, min_gain=1e-7, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.impurity_function = impurity_function\n",
    "        self.n_bins = n_bins                 \n",
    "        self.min_gain = min_gain             \n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    # --------------- ENTRENAMIENTO ----------------\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if (n_labels == 1\n",
    "            or (self.max_depth and depth >= self.max_depth)\n",
    "            or n_samples < self.min_samples_split):\n",
    "            return np.bincount(y).argmax()    \n",
    "\n",
    "        best_gain  = -1\n",
    "        best_split = None\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            col = X[:, feature]\n",
    "            thresholds = np.percentile(col, np.linspace(0, 100, self.n_bins))\n",
    "            thresholds = np.unique(thresholds)   #\n",
    "\n",
    "            for thr in thresholds:\n",
    "                mask = col <= thr\n",
    "                left_idx = y[mask]\n",
    "                right_idx = y[~mask]\n",
    "                #left_idx  = np.where(col <= thr)[0]\n",
    "                #right_idx = np.where(col >  thr)[0]\n",
    "\n",
    "                if (len(left_idx) < self.min_samples_split or\n",
    "                    len(right_idx) < self.min_samples_split):\n",
    "                    continue\n",
    "\n",
    "                gain = information_gain(y, mask, ~mask, self.impurity_function)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = (feature, thr)\n",
    "                    best_left_mask = mask\n",
    "                    best_right_mask = ~mask\n",
    "\n",
    "        if best_gain < self.min_gain or best_split is None:\n",
    "            return np.bincount(y).argmax()\n",
    "\n",
    "        f, thr = best_split\n",
    "        left = self._build_tree(X[best_left_mask], y[best_left_mask], depth + 1)\n",
    "        right = self._build_tree(X[best_right_mask], y[best_right_mask], depth + 1)\n",
    "\n",
    "\n",
    "        return {\"feature\": f, \"threshold\": thr,\n",
    "                \"left\": left, \"right\": right}\n",
    "\n",
    "    # --------------- PREDICCIÓN ----------------\n",
    "    def predict_one(self, x, node=None):\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
    "            return self.predict_one(x, node[\"left\"])\n",
    "        else:\n",
    "            return self.predict_one(x, node[\"right\"])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_one(x) for x in X])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Obtener los parámetros del modelo\"\"\"\n",
    "        return {\"max_depth\": self.max_depth, \"impurity_function\": self.impurity_function}\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Configurar los parámetros del modelo\"\"\"\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "    def saveModel(self, filename=\"best_tree.pkl\"):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
